---
layout: page
title:  "Experiments"
date:   2017-11-21
use_math: true
---

# Implementation and Experiments

### Two different architectures

Two different architectures, depicted in the following figures - let's call them SC-CON and SC-ENT - were tried.

<center><img src="content/sc-con.pdf"><font color="white">kkkkkkkkkkkk</font><img src="content/sc-ent.pdf"> </center>
<center><i><font size="3">SC-CON (left) and SC-ENT (right) architecture for the implementation of the Siamese Network.</font></i></center>

At each case, the architecture of the tied subnetworks used was the same and consisted of $3$ convolutional blocks ($32$, $64$, and $96$ filters) followed by $3$ dense blocks ($384$, $192$, and $96$ neurons). Each block had a batch normalization layer, as well as a dropout layer with $p=0.1$. The activation function used was ReLU, while the kernels were initialized based on Xavier initialization. For all the convolutional blocks, the size of the convolution window was $3\times3$ and the stride was equal to $1$ at both dimensions, while a max pooling of size $2\times2$ was applied. The corresponding python function (using the [Keras][keras] API) is:

```python
from keras.models import Model
from keras.layers import Input, Conv2D, BatchNormalization, 
                         Activation, MaxPooling2D, Dropout, 
                         Flatten, Dense

def create_subnetwork(input_shape):
  # Subnetwork to be shared by the siamese architecture.
  
  input_layer = Input(shape=input_shape)

  x = Conv2D(32, (3,3), strides=(1,1), padding='same', 
             kernel_initializer='glorot_normal', 
             kernel_regularizer=None)(input_layer)
  x = BatchNormalization()(x)
  x = Activation('relu')(x)
  x = MaxPooling2D(pool_size=(2,2))(x)
  x = Dropout(0.1)(x)

  x = Conv2D(64, (3,3), strides=(1,1), padding='same', 
             kernel_initializer='glorot_normal', 
             kernel_regularizer=None)(x)
  x = BatchNormalization()(x)
  x = Activation('relu')(x)
  x = MaxPooling2D(pool_size=(2,2))(x)
  x = Dropout(0.1)(x)

  x = Conv2D(96, (3,3), strides=(1,1), padding='same', 
             kernel_initializer='glorot_normal', 
             kernel_regularizer=None)(x)
  x = BatchNormalization()(x)
  x = Activation('relu')(x)
  x = MaxPooling2D(pool_size=(2,2))(x)
  x = Dropout(0.1)(x)

  x = Flatten()(x)
  x = Dense(384, kernel_initializer='glorot_normal')(x)
  x = BatchNormalization()(x)
  x = Activation('relu')(x)
  x = Dropout(0.1)(x)

  x = Dense(192, kernel_initializer='glorot_normal')(x)
  x = BatchNormalization()(x)
  x = Activation('relu')(x)
  x = Dropout(0.1)(x)

  x = Dense(96, kernel_initializer='glorot_normal')(x)
  x = BatchNormalization()(x)
  x = Activation('relu')(x)
  x = Dropout(0.1)(x)

  return Model(input_layer, x)
```
where for our purposes `input_shape = (128, 128, 1)`.

For the SC-CON architecture the euclidean distance between the two $96$-dimensional outputs was being computed and the network was trained based on the minimization of the contrastive loss function. Thus, the output of the system was a distance between the inputs (very dissimilar inputs were expected to yield big distances). The contrastive loss is defined as 

$$L = (1-Y)E^2 + Y\max\{0, m-E\}^2$$

where 

$$D = ||G(X_1) - G(X_2)||$$

with $X_1$, $X_2$ the two inputs and $Y$ beign equal to $0$ if the two inputs are labeled "similar" and $1$ otherwise. The margin $m$ is a design parameter which was chosen $m=1$.

For the SC-ENT architecture the two $96$-dimensional outputs were being conctenated into a $192$-dimansional layer which was being propagated successively to a $96$-dimensional dense layer, a batch normalization layer, a ReLU activation layer and, a dropout layer with $p=0.1$ and a final $1$-dimensional dense layer. A sigmoid activation function was being applied to that final neuron and the network was trained based on the minimization of the binary cross-entropy loss function. Thus, the output of the system was essentially the likelihood that the two inputs are similar or not.

The model described for those two cases can be generated by the following function:

```python
from keras.layers import Lambda, Concatenate
from keras import backend as K

def euclidean_distance(vectors):
  x, y = vectors
  return K.sqrt(K.maximum(K.sum(K.square(x - y), axis=1, keepdims=True), K.epsilon()))

def create_siamese_network(input_shape, architecture):
  subnetwork = create_subnetwork(input_shape)

  input_layer1 = Input(shape=input_shape)
  input_layer2 = Input(shape=input_shape)

  # use the same instance of subnetwork (with different inputs)
  # so, in effect use the same subnetwork (same weights) => siamese
  suboutput1 = subnetwork(input_layer1)
  suboutput2 = subnetwork(input_layer2)

  if (architecture == 'sc-con'):
    # works with Tensorflow backend
    out_distance = Lambda(euclidean_distance)([suboutput1, suboutput2])
    model = Model([input_layer1, input_layer2], out_distance)

  elif (architecture == 'sc-ent'):
    merged = Concatenate(axis=-1)([suboutput1, suboutput2])
    x = Dense(96, kernel_initializer='glorot_normal')(merged)
    x = BatchNormalization()(x)
    x = Activation('relu')(x)
    x = Dropout(0.1)(x)
    x = Dense(1, kernel_initializer='glorot_normal')(x)
    likelihood = Activation('sigmoid')(x)
    model = Model([input_layer1, input_layer2], likelihood)

  return model
```

### Two different testing approaches

Any recording where it was desired that the speaker homogeneous regions be found by the trained system had to be split into consecutive segments of the same duration as the training segments ($1.27$sec) and each segment had to be preprocessed with the same feature extraction technique appried during the training. Adjacent segments could then be compared throught the network to decide whether the segments belong to the same or different speakers. "Adjacent" segments can be defined in two different ways as shown in the following figures.

<center><img src="content/Diagram3.pdf"><font color="white">kkkk</font><img src="content/Diagram42.pdf"> </center>
<i><font size="3">Two different point of views about adjacent segments. (left): The signal is windowed in overlapping segments and 2 consecutive segments form a pair. (right): The signal is windowed in overlapping segments but each pair is formed by non-overlapping segments.</font></i> 

### Evaluation Metrics
The evaluation metrics traditionally used for SCD are the False-Alarm Rate ($FAR$) and the Mis-Detection Rate ($MDR$), as well as the $precision$, $recall$, and $F_1$-score, as defined by the following formulas:

$$FAR = \frac{\text{#false_alarms}}{\text{#false_alarms + #correct_detections + #missed_detections}}$$

$$MDR = \frac{\text{#missed_detections}}{\text{#correct_detections + #missed_detections}}$$

$$precision = \frac{\text{#correct_detections}}{\text{#correct_detections + #false_alarms}}$$

$$recall = \frac{\text{#correct_detections}}{\text{#correct_detections + #missed_detections}}$$

$$F_1 = 2\frac{precision \cdot recall}{precision + recall}$$

A tolerance margin is also traditionally applied, since it is nearly impossible to find the "perfect" timestamps of a speaker change, as labeled by human annotators. This tolerance margin has been seleceted equal to $\pm0.5$sec, which is a typical value.

### Results

<center><img src="content/learning_curves.pdf" width="120%"> </center>

[keras]: https://keras.io